[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioinformatics Protocols",
    "section": "",
    "text": "Preface\nThis is a summary of commonly used pipelines and data analysis protocols used in the Yu Lab. Furthermore, it includes many the tips and tricks I’ve learned and kept track of. I consider this a mini-cookbook of sorts.\nThis book is not an exhaustive list of everything we do in the lab. For example, I do not have any experience with DiMeLo-Seq data analysis. I also do not consider this the best practices or the only possible way of doing things, but I’ve tried my best to pick the best/easiest methods. However, the code for book is published on GitHub and any lab member can contribute to it in the future and add more sections or change approaches.\nI’ve tried to follow the typical life cycle of NGS data analysis in organizing the book. We start with basic QC and processing pipelines. The chapters following are method specific analysis. At the end, I have programming tips as well as general resources that have helped me immensely in learning everything in this book. More specific resources are provided where relevant.\nI try to keep this short and not include all of the background for certain decisions made. Some of the code included may seem very trivial but I would’ve wished I had the handy snippets when I started out. I also assume you already know some basics.\nThis book was written in Quarto. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "pipelines.html#working-with-high-performance-computing-hpcs",
    "href": "pipelines.html#working-with-high-performance-computing-hpcs",
    "title": "1  Glueing Together Pipelines",
    "section": "1.1 Working with High Performance Computing (HPCs)",
    "text": "1.1 Working with High Performance Computing (HPCs)\nNorthwestern’s Quest User Guide is a great place to start with learning how HPCs work. However, with our own server at Emory, things are much simpler.\nCancel a lot of pending jobs scancel --state PENDING --user vkp2256."
  },
  {
    "objectID": "pipelines.html#environment-reproducibility",
    "href": "pipelines.html#environment-reproducibility",
    "title": "1  Glueing Together Pipelines",
    "section": "1.2 Environment Reproducibility",
    "text": "1.2 Environment Reproducibility\nMany packages will now use conda, so many of those will be self-contained. I highly recommend using mamba to speed up installations.\nFor R, while one can use a conda environment, I’ve found it to be hard to set it up, so try to use only one version of R in one project to avoid dependency issues and do sessioninfo() to get package versions that were installed."
  },
  {
    "objectID": "pipelines.html#nextflow",
    "href": "pipelines.html#nextflow",
    "title": "1  Glueing Together Pipelines",
    "section": "1.3 Nextflow",
    "text": "1.3 Nextflow\nNextflow is a pipeline construction language. Nf-core is a publicly contributed best-practices of bioinformatics pipelines. If you want to do basic, routine analysis, using nf-core’s ready-made pipelines is best, but because of its large code-base and complexity, it might be hard to debug. You may want to start your own nextflow container if you find yourself doing similar analyses many times.\nUse nextflow pull nf-core/rnaseq to update pipelines. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since."
  },
  {
    "objectID": "pipelines.html#the-command-line",
    "href": "pipelines.html#the-command-line",
    "title": "1  Glueing Together Pipelines",
    "section": "1.4 The Command Line",
    "text": "1.4 The Command Line\nIt is essential to master the basics of the command line. (Buffalo 2015, chap. 8) is a great place to get started. I found learning about path expansion, awk, and vim extremely useful. The Missing Semester is also a great resource.\nMake your executable bash script have a help page by creating a Help facility.\nWhile I think going through these well-documented resources is better than anything I could share, I’ll document the things I found really useful here.\n\n\n\n\n\n\nWarning\n\n\n\nSome of these might not be accurate, test carefully before using them (as you should do with any random piece of code you find).\n\n\n\n1.4.1 One-liners\nThere are many great bioinformatics one-liner compilations such as this one. I suggest you keep a list of your own that you find yourself re-using often.\n\n\n1.4.2 File manipulation\nRead in files line by line and perform an action with it.\n\nfile = \"/path/to/yourfile.txt\"\nwhile IFS= read -r line\ndo\necho \"$line\"\necho \"${line:8:5}\" # grab chars starting at 8th position(0 index) for 5 chars\ndone &lt;\"$file\"\n\nIf you want to refer to columns in your file\n\n# Loop through each line in the mapping file and rename the files\nwhile IFS=' ' read -r old_name new_name; do\n    # Rename the files\n    mv \"$old_name\" \"$new_name\"\ndone &lt; \"$mapping_file\"\n\nChecking if a directory already exists\n\nif [ ! -d $dir ]; then\n    mkdir $dir\nfi\n\nFor loops: there are many ways of specifying what to loop through\n\n#!/bin/bash\nfor i in m{1..3} m{13..15}\nfor i in 1 2 3 4 5\nfor i in file1 file2\n\ndo\necho \"Welcome $i times\"\nj=\"$(basename \"${i}\")\" # get the file name\n\nk=\"$(basename \"${i}\" | sed 's/_[^_]*$//')\" # prints file name without the extension and the last chunk after the last _\nl=\"$(dirname \"${i}\")\" # prints the dirname without filename\necho raw/JYu-${i}_*_L001* # filename expansion\n\ndone\n\nFor more complex naming schemes, you may use an array.\n\n# Specify the last two digits of the numbers\nnumbers=(\"59\" \"82\" \"23\" \"45\" \"77\")\n\n# Iterate over the array\nfor num in \"${numbers[@]}\"; do\n  echo \"SRR13105$num\"\ndone\n\n# Iterate over indexes\nfor ((index=0;index&lt;5;index++))\ndo\nnum=\"SRR13105${numbers[$index]}\"\ndone\n\nSee this for more examples and use cases.\n\n\n1.4.3 Unix Text Processing: Awk and Sed\nAwk\nAwk statements are written as pattern { action }. If we omit the pattern, Awk will run the action on all records. If we omit the action but specify a pattern, Awk will print all records that match the pattern.\n\nawk '{if ($3 == \"sth) print $0;}' file.tsv\nawk -F \"\\t\" '{print NF; exit}' file.tsv # prints number of fields/columns\nawk -F \"\\t\" 'BEGIN{OFS=\"\\t\";} {print $1,$2,$3,\"+\"}' input.bed &gt; output.bed # indicate that the output should be tab-delimited\ncolor=\"$(awk 'NR==val{print; exit}' val=$line color_list_3_chars.txt)\" # pick line based on counter and assigning to a variable\n\nSed\nSed statements are written as substitute: 's/pattern/replacement/'\nsed -e some rule -e another rule The g/ means global replace i.e. find all occurrences of foo and replace with bar using sed. If you removed the /g only first occurrence is changed. chaining rules also possible: sed -e ‘s/,/g ; 5q’ print only the first 5 lines\n\n\n1.4.4 Miscellanous\n\nInside a bash script, you can use $0 for the name of the script, $1 for the first argument and so on.\n\n$? show error code of last command, 0 if everything went well, $# number of commands.\nshuf -n 4 example_data.csv print random 4 lines.\n\nnohup command & will make commmand run in the background, fg to bring it back to the foreground.\n\nhttps://www.shellcheck.net/ is a website where it can check for errors and bugs in your shell scripts.\npaste -sd, concatenate lines into a single line -s, delimited by a comma\n\n\n1.4.4.1 Working with lines\ncount number of occurrences of unique instances\n\nsort | uniq -c \n\noutput number of duplicated lines if there are any\n\nuniq -d mm_gene_names.txt | wc -l\n\ncount fastq lines (assuming well-formatted fastq files which it usually are)\n\ncat file.fq | echo $((`wc -l`/4))\n\nprint only the number of lines in a file and no filenames\n\nwc -l m*/m*.narrowPeak | cut -f 4 -d' '\n\nto get rid of x lines at the beginning of a file\n\ntail -n +2 file.txt # starts from the 2nd line, i.e. removing the first line\n\nto see the first 2 lines and the last 2 lines of a file\n\n(head -n 2; tail -n 2) &lt; Mus_musculus.GRCm38.75_chr1.bed\n\n\n\n1.4.4.2 Working with columns\n\ncut -f 2 Mus_musculus.GRCm38.75_chr1.bed # only the second column\ncut -f 3-8 Mus_musculus.GRCm38.75_chr1.bed # range of columns  \ncut -f 3,6,7,8 Mus_musculus.GRCm38.75_chr1.bed # sets of columns, cannot be reordered  \ncut -d, -f2 some_file.csv # use comma as delimiter\n\ngrep -v \"^#\" some_file.gtf | cut -f 1-8 | column -t | head -n 3 #prettify output\n\nremove header lines and count the number of columns\n\ngrep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | awk -F \"\\t\" '{print NF; exit}'\n\n\n\n1.4.4.3 Working with strings\nto remove everything after first dot. useful for getting sample name from filename.fastq.gz\n\n${i%%.*}\n\nto remove everything after last dot. useful for getting sample name from filename.param.fastq\n\n${i%.*}\n\nto remove everything before a /, including it\n\n${i##*/}\n\nto make uppercase\n\n${var^}\n\n\n\n1.4.4.4 Directories\nto get human readable size of folders under the parent folder\n\ndu -h --max-depth=1 parent_folder_name\n\ns means summary\n\ndu -sh /full/path/directory\n\nprint each file in new line\n\nls -1\n\ncheck what file takes the most space (d is one level down)\n\ndu -d 1 | sort -n\n\n\n\n1.4.4.5 Alignment files\nRemoving EBV chromosomes for viewing on UCSC browser.\n\nsamtools idxstats m${i}_sorted.bam | cut -f 1 | grep -v 'chrEBV' | xargs samtools view -h m${i}_sorted.bam | grep -v 'chrEBV' | samtools view -o m${i}_filtered.bam\n\nRemove reads not mapping to chr1-22, X, Y. this does not remove from headers. The first sed expression removes leading whitespace from echo (-n to ), the second expression to add “chr” at the beginning.\n\nsamtools view -o mr392_filtered1.bam mr392_sorted.bam `echo -n {{1..22},X,Y}$'\\n' | sed -e 's/^[[:space:]]//' -e 's/^/chr/'`\n\nor you can just chain together reverse grep to remove any chromosomes you want\n\nchr=samtools view -H m1076_rgid_reheader_st.bam | grep chr | cut -f2 | sed 's/SN://g' | grep -v chrM | grep -v chrY | awk '{if(length($0)&lt;6)print}'\n\nthe awk statement is to remove the unknown chromosomes and random contigs since they will be longer than 6 chars"
  },
  {
    "objectID": "pipelines.html#bioinformatics-bits",
    "href": "pipelines.html#bioinformatics-bits",
    "title": "1  Glueing Together Pipelines",
    "section": "1.5 Bioinformatics Bits",
    "text": "1.5 Bioinformatics Bits\nThese are mostly still command line tools, but more bioinformatics related.\n\n1.5.1 Getting files from GEO\nUsing SRAtools https://bioinformaticsworkbook.org/dataAcquisition/fileTransfer/sra.html#gsc.tab=0 To get the SRR runs numbers, use the SRA Run Selector. Select the ones you want to download or all of them, and download the Accession List. Upload to quest. Better to do a job array for each SRR. fasterq-dump is now the preferred and faster option. To download bam, can bystep sam step.\n\nmodule load sratoolkit\nfastq-dump --gzip SRR # use --split-files for paired-end\nfasterq-dump SRR -O output/dir # for 10x, need to use --split-filles and --include-technical\nsam-dump SRR | samtools view -bS -&gt; SRR.bam\n\nUsing ftp\nThis can be faster than SRAtoolkit but only works if those files have been uploaded to EBI.\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR101/006/SRR1016916\npattern: fastq/first 6 chars/00(last number)/full accession\n\n\n1.5.2 Making UCSC tracks\n\nmdoule load homer\nmakeTagDirectory tag1045 1045.bed\nmakeUCSCfile tagm57 -o m57 -norm 2e7v\n\nWith RNA-Seq UCSC tracks, use –fragLength given, otherwise it messes up the auto-correlation thinking that’s it’s ChIP-Seq leading to screwed up tracks.\nYou can upload the tracks and save it as a session that can be shared.\n\n\n1.5.3 samtools\nLearn the Bam Format.\n\nsamtools view -h test.bam # print bam with headers\nsamtools view -H test.bam # headers only\nsamtools flagstat test.bam # get mapping info\nsamtools idxstats test.bam\nsamtools -f 0x2 # read mapped in proper pair\nsamtools view -H test.bam | grep @HD # Check if BAM files are sorted\nsamtools view -F 4 test.bam | wc -l # check how many mapped reads"
  },
  {
    "objectID": "pipelines.html#r-quirks",
    "href": "pipelines.html#r-quirks",
    "title": "1  Glueing Together Pipelines",
    "section": "1.6 R Quirks",
    "text": "1.6 R Quirks\nWhen saving anything produced with ggplot and you will be editing in Illustrator, set useDingbats = FALSE.\n\n\n\n\n\n\nBuffalo, V. 2015. Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media. https://books.google.com/books?id=XxERCgAAQBAJ."
  },
  {
    "objectID": "bulk.html#rna-seq",
    "href": "bulk.html#rna-seq",
    "title": "2  Bulk Analysis",
    "section": "2.1 RNA-Seq",
    "text": "2.1 RNA-Seq\nWhile RPKM has some issues and is not the most correct quantification, for basic comparison, this is good enough to share with biologists. The nf-core pipeline will provide this value by default.\n\n2.1.1 Differential Gene Expression\nOne of the most basic and common analysis on RNA-Seq. We use DESeq2 and their well documented vignette is worth reading from start to end for the beginner.\n\n\n\n\n\n\nNote\n\n\n\nWhile our pipeline originally used the STAR --quantMode to quantify genes, with switching to nf-core, we are also switching to STAR alignment followed by RSEM quantification.\n\n\n\nlibrary(tidyverse)\n\nfiles &lt;- list.files(path = \"./working_dir\", pattern = \"*genes.results$\", full.names = TRUE)\nrsem_results &lt;- lapply(files, read_delim)\nexpected_counts_list &lt;- lapply(rsem_results, function(x) { x$expected_count })\nexpected_counts &lt;- do.call(cbind, expected_counts_list) %&gt;% as.data.frame()\n\nRSEM produces non-integer counts, and we can by-pass that by using round(). Alternatively, you can use tximport to read the files in.\n\nexpected_counts &lt;- round(expected_counts) \nrownames(expected_counts) &lt;- rsem_results[[1]]$gene_id\ncolnames(expected_counts) &lt;- stringr::str_extract(files, \"mr\\\\d+\") # our RNA-seq samples usually start with MR\n\nsampleTable &lt;- data.frame(condition = factor(rep(c(\"control\", \"knockdown\"), each = 3)),\n                          replicate = factor(rep(seq(1,3))))\nrownames(sampleTable) &lt;- colnames(expected_counts)\n\nDESeq2\n\ndds &lt;- DESeqDataSetFromMatrix(expected_counts, sampleTable, design = ~condition)\nkeep &lt;- rowSums(counts(dds)) &gt; 10\ndds &lt;- dds[keep,]\ndds &lt;- DESeq(dds)\nres &lt;- results(dds, alpha = 0.01)\nsummary(res)\n\nDifference between rlog, vst and lfcShrink https://support.bioconductor.org/p/104615/.\nPlotting PCs\nDESeq2’s plotPCA() function will plot the top 500 most variable genes. The chunk below will plot all genes.\n\ndegenes &lt;- res %&gt;% subset(padj &lt; 0.01)\ndds_rlog &lt;- rlog(dds)\npca_data &lt;- t(assay(dds_rlog)) %&gt;% prcomp()\nautoplot(pca_data, data = sampleTable, colour = \"condition\") + \n  geom_text_repel(label = rownames(sampleTable))\nrlog_de &lt;- assay(dds_rlog) %&gt;% subset(rownames(dds_rlog) %in% rownames(degenes))\nrlog_de_scaled &lt;- t(scale(t(rlog_de)))\n\nAfter getting DEGs, you’d want to group the genes into biological functions. See Section 3.2 for Over representation analysis (ORA) with GO and KEGG terms as well as Gene Set Enrichment Analysis (GSEA) with ranked genes.\n\n\n2.1.2 Deciding Groups and Plotting\nWhile you can use k-means manually to get seperate groups, ComplexHeatmap allows you to do so with more flexiblity and get visualizations as well.\n\nmat_colors &lt;- list(\n  replicate = c(brewer.pal(3, \"Accent\")),\n  condition = c(brewer.pal(6, \"Set1\")))\nnames(mat_colors$replicate) &lt;- unique(sampleTable$replicate)\nnames(mat_colors$condition) &lt;- sampleTable$condition\n\ncol_anno &lt;- HeatmapAnnotation(df = sampleTable,\n                              which = 'col',\n                              col = mat_colors\n)\n\nhmap &lt;- Heatmap(rlog_de_scaled,\n                   name = \"scaled\",\n                   \n                   # Row Params\n                   show_row_names = FALSE,\n                   row_title_rot=0,\n                   cluster_row_slices = FALSE,\n                   border = TRUE,\n                   row_km = 2, # split rows into 2 groups\n                   \n                   # Column Params\n                   cluster_columns = FALSE,\n                   column_title = \"Rlog Transformed Expression for all DE genes\",\n                   top_annotation = col_anno)\nhmap &lt;- draw(hmap) # assigning so that k-means is only called once\nrow_order(hmap) # grab the different groups in rows\n\nSee here for why we assign draw()."
  },
  {
    "objectID": "bulk.html#chip-seq-and-atac-seq",
    "href": "bulk.html#chip-seq-and-atac-seq",
    "title": "2  Bulk Analysis",
    "section": "2.2 ChIP-Seq and ATAC-Seq",
    "text": "2.2 ChIP-Seq and ATAC-Seq\n\n2.2.1 Spike-in normalization\nGuidelines on spike-in normalization from ActiveMotif.\n\nPerform ChIP combining the Spike-in Chromatin, Spike-in Antibody, test chromatin and test antibody into the same tube for immunoprecipitation. We suggest using the guidelines provided for chromatin and antibody quantities based on the antibody target.\nFollow ChIP with Next-Generation Sequencing.\nMap ChIP-seq data to the test reference genome (e.g. human, mouse or other).\nMap ChIP-seq data to the Drosophila reference genome.\nCount uniquely aligning Drosophila sequence tags and identify the sample containing the least number of tags.\nCompare Drosophila tag counts from other samples to the sample containing the least tags and generate a normalization factor for each comparison. (Sample 1 with lowest tag count / Sample 2) = Normalization factor\nDownsample the tag counts of data sets proportional to the normalization factor determined\n\n\n\n2.2.2 Peak calling\nDepending on if you use the nfcore’s pipeline or your own, you will have to call peaks. I use MACS2 and here are some details I’ve gathered.\nhttps://github.com/crazyhottommy/ChIP-seq-analysis/blob/master/part1.3_MACS2_peak_calling_details.md\nWhy I skip model building https://github.com/macs3-project/MACS/issues/391\n\nmacs2 callpeak -t mxx_sorted.bam --outdir macs/mxx -n mxx -g hs -q 0.01 --nomodel --shift 0 --extsize 250\n\n\n\n2.2.3 Overlapping peaks\nAfter MACS2 peak-calling, we may want to see how many peaks overlap in different conditions. Even though it’s named mergePeaks, you will be able to get overlapping statistics from this. Be mindful of long path names as mergePeaks will produce errors.\n\nmergePeaks -d 100 pu1.peaks cebp.peaks -prefix mmm -venn venn.txt\n\n\n\n\n\n\n\nTip\n\n\n\nThe -d flag changes the unique peaks to 100 bp each and keep the shared peaks same size.\nTo get literal 1bp overlap, just omit -d argument altogether\n\n\nThe easiest way to plot this result is with Vennerable.\n\na &lt;- tot[1]\nb &lt;- tot[2]\nab &lt;- tot[3]\n\nvenn_obj &lt;- createVennObj(nSets = 2, sNames = c(\"m1043\", \"m1044\"), # names in order of first to last\n                     sSizes = c(0, a, b ,ab))\nvp &lt;- plotVenn(nVennObj = venn_obj)\n\nThis gets tedious with more than 2 sets. See code below. You should not make Venn Diagrams with more than 4 sets. An upset plot is better in that scenario.\n\n\nShow the code\n# 3 sets -----\na &lt;- tot[1]\nb &lt;- tot[2]\nab &lt;- tot[3]\nc &lt;- tot[4]\nac &lt;- tot[5]\nbc &lt;- tot[6]\nabc &lt;- tot[7]\n\n# 4 sets -------\na &lt;- tot[1]\nb &lt;- tot[2]\nab &lt;- tot[3]Ve\nc &lt;- tot[4]\nac &lt;- tot[5]\nbc &lt;- tot[6]\nabc &lt;- tot[7]\nd &lt;- tot[8]\nad &lt;- tot[9]\nbd &lt;- tot[10]\nabd &lt;- tot[11]\ncd &lt;- tot[12]\nacd &lt;- tot[13]\nbcd &lt;- tot[14]\nabcd &lt;- tot[15]\n\n\nThis is a more automated way of reading in the data, here using 4 sets as an example.\n\nvenn &lt;- read.table(\"results/output_nepc.bedpe\", header = TRUE, sep = \"\\t\")\nvenn$alpha &lt;- apply(venn, 1, function(x) {\n  sets &lt;- c(\"A\", \"B\", \"C\", \"D\")\n  selected_sets &lt;- sets[which(x == \"X\")]\n  paste(selected_sets, collapse = \"\")\n})\n\norder_vector &lt;- c(\"0\", \"A\", \"B\", \"AB\", \"C\", \"AC\", \"BC\", \"ABC\", \"D\", \"AD\", \"BD\", \"ABD\", \"CD\", \"ACD\", \"BCD\", \"ABCD\")\n\nvenn$alpha &lt;- factor(venn$alpha, levels = order_vector)\n\n# Sort the data frame based on the factor levels\nvenn &lt;- venn[order(venn$alpha), ]\nvenn_plot &lt;- Venn(SetNames = c(\"93\", \"145.1\", \"145.2\", \"nci\"), \n                  Weight = c(0, venn$Features))\nplot(venn_plot, type = \"ellipses\")\n\nCreating upset plots https://github.com/hms-dbmi/UpSetR\n\nlibrary(UpSetR)\nvenn$Sets &lt;- apply(venn[, -1], 1, function(x) {\n  sets &lt;- colnames(venn)[2:17]\n  selected_sets &lt;- sets[which(x == \"X\")]\n  paste(selected_sets, collapse = \"&\")\n})\n\nupset_input &lt;- c(venn$Features)\nnames(upset_input) &lt;- venn$Sets\nupset(fromExpression(upset_input), nsets = 16, nintersects = 100, number.angles = 45)\n\nA sample script to automate this process. I haven’t incorporated the above part into this script.\n\n\n\nmergePeaksandplot.sh\n\nmodule load homer/4.10\nmodule load R/4.0.3\nmergePeaks control_peaks.narrowPeak purturbed_peaks.narrowPeak -prefix ov -venn venn.txt\n# n_way script, venn.txt, last sample label, middle samples label, first sample label, txt file name (no extension)\nRscript --vanilla venn2way.R venn.txt purturbed control venn\n\n\nThe accompanying R file:\n\n\n\nvenn2way.R\n\nlibrary(Vennerable)\n\nargs &lt;- commandArgs(trailingOnly = TRUE)\n\nvenn &lt;- read.table(args[1], header = TRUE, sep = \"\\t\")\ntot &lt;- venn[[Total]]\n\na &lt;- tot[1]\nb &lt;- tot[2]\nab &lt;- tot[3]\n\nvenner &lt;- Venn(SetNames = c(args[3], args[2]), # opposite labelling to nVennR, going from last sample to first\n               Weight = c(0, b, a, ab))\npng(paste0(args[4], \".png\"))\nplot(venner)\ndev.off()\n\n\nTo make Venn Diagrams that are more accurately weighted, use nVennR. Sadly, it looks like it’s been removed from CRAN since the last time I’ve installed it.\nIf you have replicates, overlapping peaks can be obtained via packages like MAnorm2. Read more about dealing with replicates here.\n\n\n2.2.4 Motif Enrichment\nHomer mergePeaks is able to take the narrowPeaks format as input, but if you’re doing homer mergePeaks first and then findMotifs, you need to change the Homer mergePeaks output to exclude the header line first. This also applies to deepTools heatmaps.\n\nfindMotifsGenome.pl peak_or_bed /projects/p20023/Viriya/software/Homer4.10/data/genomes/hg38/ output_dir -size 200 \n\n-size 200 is the default, and is calculated from the center of the peaks. If your peaks are bigger and you wish to use the entire region, use -size given. However, when the regions are too large, motifs will not be significantly enriched.\n\n\n2.2.5 Plotting Heatmaps\nHere I’m showing a sample deepTools script to plot heatmaps. First you need bigwig files. bamCoverage offers normalization by scaling factor, Reads Per Kilobase per Million mapped reads (RPKM), counts per million (CPM), bins per million mapped reads (BPM) and 1x depth (reads per genome coverage, RPGC).\n\nbamCoverage --bam m${i}_sorted.bam --outFileName m${i}.bw --normalizeUsing CPM \\\n--extendReads 200 --numberOfProcessors 6 --binSize 20\n\nI find having paths defined at the top of the script makes it less likely to have mistakes.\n\nday3=\"m1015_1055_hg38/foxa2/CPM\"\nnci=\"m1015_1055_hg38/CPM\"\nchip=\"../Viriya/analysis/foxa2/chip-seq\"\n\ncomputeMatrix reference-point --referencePoint center -S \\\n$day3/m674.bw $day3/m676.bw $tf/m914.bw $tf/m921.bw $nci/m1043.bw $nci/m1044.bw \\\n-R $chip/clustering/output/1.bed \\\n$chip/clustering/output/2.bed \\\n$chip/clustering/output/3.bed \\\n$chip/clustering/output/4.bed \\\n-a 3000 -b 3000 -o $chip/heatmaps/scaled_FOXA2_sites.npz \\\n--samplesLabel m674 m676 m914 m921 m1043 m1044 \\\n-p max --blackListFileName $chip/ENCFF356LFX.bed.gz\n\nplotHeatmap -m $chip/FOXA2_6_samples/heatmaps/scaled_FOXA2_sites.npz \\\n-o $chip/FOXA2_6_samples/heatmaps/scaled_FOXA2_sites.pdf \\\n--colorMap Blues\n\n--sortUsingSamples is not 0 indexed. 1st sample is 1. --sortUsingSamples is also usable in plotHeatmap so we can save that to there for more flexible plotting.\n--regionsLabel: “xxx binding sites n=2390” put it in quotes if you don’t want to key in escape too many times\n--sampleLabels: if you’re sure you’re not going to change any labels, put it in makeheatmap step because fewer things to type when redoing the heatmap multiple times due to scaling etc, therefore fewer mistakes. otherwise put it in plotHeatmap.\nIn legends: “Pol II” spaces and parantheses need to be escaped, but not + signs.\nUse BED6 format annd not the BED3 if you care about strandedness https://github.com/deeptools/deepTools/issues/886.\nTo get sorted output bed files, use the --sortedOutRegions at the plotHeatmap step, not computeMatrix.\nUse --clusterUsingSamples for a more robust delineation.\n\n\n2.2.6 Genomic Annotation\nChIPSeeker\n\nlibrary(org.Hs.eg.db)\nlibrary(TxDb.Hsapiens.UCSC.hg38.knownGene)\n\npromoter &lt;- getPromoters(TxDb=txdb, upstream=3000, downstream=3000)\n\nmxxx &lt;- readPeakFile(\"/path_to_narrowPeak/mxxx_peaks.narrowPeak\")\ntagMatrix &lt;- getTagMatrix(mxxx, windows = promoter) # usually needs an interactive job\npeakAnno &lt;- annotatePeak(mxxx, tssRegion = c(-3000,3000), TxDb = txdb, annoDb = \"org.Hs.eg.db\")\nplotAnnoPie(peakAnno, main = \"mxxx\", line = -6)\nvennpie(peakAnno, r = 0.1)\nupsetplot(peakAnno) + ggtitle(\"mxxx\")\nplotAnnoBar(peakAnno)\n\nannotatePeak will default to chosing one gene per region. This is fine for small binding sites but for larger regions use seq2gene as it will map genomic regions in a many-to-many manner."
  },
  {
    "objectID": "functional.html#gene-name-conversions",
    "href": "functional.html#gene-name-conversions",
    "title": "3  Functional Analysis",
    "section": "3.1 Gene name conversions",
    "text": "3.1 Gene name conversions\nGetting genomic coordinates from a list of gene names.\n\nensembl &lt;- useEnsembl(biomart = \"ensembl\", dataset=\"hsapiens_gene_ensembl\", version = 86)\nchr &lt;- c(seq(1,22), \"X\", \"Y\")\n\ngene_coor &lt;- getBM(attributes = c(\"hgnc_symbol\", \"chromosome_name\", \"start_position\", \"end_position\"), \n                    filters = \"hgnc_symbol\", values = rownames(res_sh_vs_ctrl), mart = ensembl,\n                    useCache = FALSE)\n# patched chromosomes are coming up, therefore not unique, need to filter out\ngene_coor &lt;- gene_coor[gene_coor$chromosome_name %in% chr,]\n\nThere are many ways of doing this, they’re all slightly different depending on the database."
  },
  {
    "objectID": "functional.html#sec-enrichment",
    "href": "functional.html#sec-enrichment",
    "title": "3  Functional Analysis",
    "section": "3.2 Enrichment Analysis",
    "text": "3.2 Enrichment Analysis\nMany of the functional analysis is achieved using ClusterProfiler. While the vignette is very comprehensive, it can be overwhelming to see which functions are useful, so I will point out the most useful ones here.\n\n3.2.1 Overrepresention Analysis (ORA)\nA very good explanation and accompanying paper of why we need background lists. Which genes to use for backgrounds? Check this.\n\ngenes &lt;- as.data.frame(peakAnno)$geneId # get the genes from peak annotation\n\ngeneUniverse &lt;- rownames(res_lrt) # all genes that were tested for significance\ngo &lt;- enrichGO (gene = genes,\n          keyType = \"SYMBOL\",\n          universe = geneUniverse,\n          ont = \"BP\",\n          OrgDb = org.Hs.eg.db,\n          qvalueCutoff = 0.05)\n\n\ngo_simplified &lt;- simplify(go) # to get rid of redundant GO terms\n\nTo wrap long labels into a few lines\n\ndotplot(go) + scale_y_discrete(labels = function(x) str_wrap(x, width = 30)) + ggtitle(\"\")  + \n  theme(plot.title = element_text(hjust = 0.5))\n\nTo use KEGG, you need to convert gene symbols to entrezid or use the geneId column from annotatePeaks.\n\ndeg_list_entrez &lt;- lapply(deg_list, function(x) {\n  mapIds(org.Hs.eg.db, rownames(x), \"ENTREZID\", \"SYMBOL\")\n})\nkegg &lt;- enrichKEGG(gene = deg_list_entrez$up, organism = \"hsa\")\n\nOr use bit_kegg https://guangchuangyu.github.io/2016/05/convert-biological-id-with-kegg-api-using-clusterprofiler/. I have not tried this method yet.\n\n\n3.2.2 Gene Set Enrichment Analysis (GSEA)\nExplanation of GSEA and how to do it in R https://sbc.shef.ac.uk/workshops/2019-01-14-rna-seq-r/rna-seq-gene-set-testing.nb.html.\nHow I first did it https://stephenturner.github.io/deseq-to-fgsea/.\nI rank the genes by the stat column. For the Wald test, stat is the Wald statistic: the log2FoldChange divided by lfcSE.\n\nranked_genes &lt;- degenes %&gt;% as.data.frame() %&gt;% rownames_to_column(\"symbol\") %&gt;% dplyr::select(symbol, stat)\nranked_genes &lt;- ranked_genes %&gt;% arrange(-stat) %&gt;% deframe()\n\nWe usually perform GSEA with the Molecular Signature Database and there is a package for easy retrieval.\n\nlibrary(msigdbr)\nh &lt;- msigdbr(species = \"Homo sapiens\", category = \"H\") %&gt;% \n  dplyr::select(gs_name, gene_symbol)\ngsea &lt;- GSEA(ranked_genes, TERM2GENE = h)\nridgeplot(gsea)\nenrichplot::gseaplot2(gsea, geneSetID = \"HALLMARK_\")"
  },
  {
    "objectID": "3D.html#resources",
    "href": "3D.html#resources",
    "title": "4  3D Chromatin Organization",
    "section": "4.1 Resources",
    "text": "4.1 Resources\nComparative study on chomatin loop callers"
  },
  {
    "objectID": "3D.html#processing",
    "href": "3D.html#processing",
    "title": "4  3D Chromatin Organization",
    "section": "4.2 Processing",
    "text": "4.2 Processing\nWe have processed all our samples with runHiC from Feng Yue’s lab as that was my committee member and expert on HiC. Many people use HiC-Pro but I have tested it and it is much slower. The one time I tried nf-core’s HiC pipeline for Micro-C, it failed.\nEach sample will require it’s own folder, with this structure.\nsample\n        data\n                hg38\n                HiC-gzip\n        workspace\n                datasets.tsv\nAll fastq files will be in HiC-gzip, with _R1.fastq.gz and _R2.fastq.gz changed to _1.fastq.gz and _2.fastq.gz. I haven’t found a workaround to avoid the renaming. The datasets.tsv, technical replicates and biological doesn’t really matter but if you want the end file to combine everything, add rep1, rep2 per pair of fastq files so the final resulting file will be denoted allReps. Do runHiC quality to get a sense of how well the experiment worked after shallow sequencing."
  },
  {
    "objectID": "3D.html#downstream-analysis",
    "href": "3D.html#downstream-analysis",
    "title": "4  3D Chromatin Organization",
    "section": "4.3 Downstream analysis",
    "text": "4.3 Downstream analysis\nMany of these have many tools available to call them. I have attempted to choose the most well-supported tools in the same ecosystem to avoid unnecessary format changes and hidden mistakes. I ended up with the Open2C ecosystem’s cooltools and many of Dr. Feng Yue’s lab’s tools.\n\n4.3.1 A/B Compartments\nI’m following the cooltools compartments tutorial.\nCompartments are not comparable between cell types if done using PCA. You can use cscore if a more robust definition is needed. However, the PCA definition is currently still widely used. For this, you need too calculate expected values first and you should output those as a tsv because you’ll need it for other analysis.\nlucap_35cr_cis_eigs = cooltools.eigs_cis(\n                        lucap_35cr,\n                        gc_cov,\n                        n_eigs=3,\n                        )\nlucap_35cr_eigenvector_track = lucap_35cr_cis_eigs[1][['chrom','start','end','E1']]\n\n\n4.3.2 Topological Associating Domain\nThere are many TAD calling tools and most of them do not have a great degree of overlap. I chose the insulation score methods in cooltools.\nCreating TADs from insulation scores https://github.com/open2c/cooltools/issues/453.\nwindows = [3*resolution, 5*resolution, 10*resolution, 25*resolution]\nsamples = {\"LuCaP70CR\" : lucap70cr_insulation_table, \n           \"LuCaP77CR\" : lucap77cr_insulation_table,\n          \"LuCaP1451\" : lucap1451_insulation_table}\nfor window in windows:\n    for table_name, table in samples.items():\n        print(\"sample\", table_name, \"at\", window)\n        insul = table[[\"chrom\", \"start\", \"end\", f\"log2_insulation_score_{window}\"]]\n        insul.to_csv(f\"results_{window}/{table_name}_insulation_scores_{window}.bed\", header=False, index=False, sep=\"\\t\")\n        \n        tads = bioframe.merge(table[table[f\"is_boundary_{window}\"] == False])\n        tads = tads[(tads[\"end\"] - tads[\"start\"]) &lt;= 1500000].reset_index(drop=True) # dropping large tads\n        tads.to_csv(f\"results_{window}/{table_name}_TADs_{window}.bed\", header=False, index=False, sep=\"\\t\")\n\n\n\n4.3.3 Loops\n\n4.3.3.1 Mustache\nUse mustache for loop calls of individual samples, and diffMustache for pair-wise comparisons. The default d for diffMustache is 2,000,000bp. Call at various cut-offs with -pt2 and decide on a reasonable number of loops.\n\n\n4.3.3.2 Working with loops\nhomer merge2Dbed.pl will combine seperate loop calls (also TADs if specified in parameter) that are at adjacent pixels (or within distance specified) into a big loop. This will also produce condition specific loops that are just based on coordinates, not statistical difference.\nCurrently, I’m using LoopRig as the underlying data structure for working with bedpe files. It’s really just 2 Granges objects linked together and I’ve re-written many of their functions. It’s most likely unnecessary to use this package.\n\n\n\n4.3.4 APA\nThere are also many implementations of APA calculation. I use coolpup.py, an extension of cooltools pileup."
  },
  {
    "objectID": "3D.html#higlass",
    "href": "3D.html#higlass",
    "title": "4  3D Chromatin Organization",
    "section": "4.4 HiGlass",
    "text": "4.4 HiGlass\nYou either start with a local installation of Docker, or use the online browser at resgen. They have some slight differences.\nFor HiGlass, you need to ingest most files in a two-step manner, whereas with resgen you can directly upload them (with the exception of TADs). It’s better to write scripts to facilitate and automate the ingestion.\nThe files will not line up perfect with the heat maps due to how HiGlass interprets coordinates. See https://github.com/higlass/higlass/issues/1051. Since HiGlass doesn’t have a notion of a genomic assembly, you need to ingest a chromosome size file once when you start.\n\n4.4.1 Docker\nInstall docker following these instructions.\nTo view HiGlass on docker locally, start the docker app, run higlass-manage start on your terminal. Go to http://localhost:8989/app to see the browser. Deletion of ingested tracks needs access to the superuser via http://localhost:8989/admin/. Once data is ingested, it’s kept at a different location and you can usually delete the original files in your local computer to save space.\n\n\n4.4.2 Resgen\nYou can create separate projects. Files uploaded needs to be manually tagged. Most important are assembly:hg38, datatype, and filetype. To use gene search, both chromosome info and gene annotations need to be added to the panel using the search bar.\n\n\n4.4.3 Navigating HiGlass\nYou need to be able to reproduce each view so you won’t have to start from scatch each time. In Docker, you save the View Configs files manually. In resgen, you can save a view.\nWhile the GUI is sufficient, some things are faster and easier changed via the view configs.\nTo change 1D scales, https://github.com/higlass/higlass/issues/946.\n\n\n4.4.4 Ingesting files specifications\nInstructions can be found here but it’s sparse and somewhat confusing so I’m including common use cases.\nAfter aggregation (whether that’s needed depends on the filetype), ingest files like this with appropriate tags.\nhiglass-manage ingest file --filetype bed2ddb --datatype 2d-rectangle-domains --project-name $3 --assembly hg38\n\n4.4.4.1 mcool\nThe .mcool files themselves are easily recognized with just higlass-manage ingest sample.mcool --assembly hg38.\n\n\n4.4.4.2 AB compartments\nThese files need to be converted into bigwigs first. They are more well supported than bedgraph files. Use UCSC’s bedGraphtoBigWig.\n\n\n4.4.4.3 TADs\nFor resgen, TADs need to be aggregated first, and the aggregated file is uploaded.\nj=${1%%.*}\n\nclodius aggregate bedpe \\\n    --assembly hg38 \\\n    --chr1-col 1 --chr2-col 1 \\\n    --from1-col 2 --to1-col 3 \\\n    --from2-col 2 --to2-col 3 \\\n    --output-file ${j}.beddb \\\n    $1\nSpecify track-type:linear-2d-rectangle-domains and filetype:bed2ddb for top section. For viewing in center section datatype:2d-rectangle-domains.\n\n\n4.4.4.4 Loops\nLoops view as arcs currently only works on resgen and not the local docker. Loops on the 2D heatmap will work with docker.\n\n\n\n4.4.5 ChIP-Seq and ATAC-Seq tracks\nThese are just bigwig ingestion."
  },
  {
    "objectID": "single-cell.html#useful-links-and-resources",
    "href": "single-cell.html#useful-links-and-resources",
    "title": "5  Single-Cell Analysis",
    "section": "5.1 Useful links and resources",
    "text": "5.1 Useful links and resources\nFine-tuning UMAP visualizations https://jlmelville.github.io/uwot/abparams.html\nTSNE animation https://distill.pub/2016/misread-tsne/\nSimple explanantion on TSNE https://www.cancer.gov/about-nci/organization/ccg/blog/2020/interview-t-sne\nDifferent integration methods https://swaruplab.bio.uci.edu/tutorial/integration/integration_tutorial.html\nIllustration of how UMAPs can be misleading https://pair-code.github.io/understanding-umap/ https://blog.bioturing.com/2022/01/14/umap-vs-t-sne-single-cell-rna-seq-data-visualization/\nSingle cell best practices from the Theis lab https://www.sc-best-practices.org/preamble.html https://broadinstitute.github.io/2019_scWorkshop/index.html\nDeep explanation of Seurat’s AddModuleScore function https://www.waltermuskovic.com/2021/04/15/seurat-s-addmodulescore-function/\n\n5.1.1 Psudotime analysis\nhttps://broadinstitute.github.io/2019_scWorkshop/functional-pseudotime-analysis.html\nList of single cell pseudotime packages publishe#d https://github.com/agitter/single-cell-pseudotime\n\n\n5.1.2 Spatial Transciptomics\nhttps://scimap.xyz/tutorials/md/spatial_biology_scimap/\n\n\n5.1.3 Downloading data\nbam files uploaded to SRA will be missing certain 10x flags and will not work. You need to get the original bam files via SRA Data Access Tab and downloaded via wget and run bamtofastq.\nfor 10x, need to use –split-filles and –include-technical.\n\n\n5.1.4 CellRanger"
  },
  {
    "objectID": "single-cell.html#seurat",
    "href": "single-cell.html#seurat",
    "title": "5  Single-Cell Analysis",
    "section": "5.2 Seurat",
    "text": "5.2 Seurat\n\n5.2.1 Integration\n\napplying sctransform to each sample in the list\nselecting integration features,\nfinding integration anchors,\nintegrate data at this point, DefaultAssay is integrated, and there is an associated VariableFeatures, which really is output of SelectIntegrationFetures. rownames(integrated[[“SCT”]](scale.data?)) is the same as SelectIntegration Features again.\nhttps://github.com/satijalab/seurat/issues/6185 explanation of the different variable gene options from sct integrated models\n\nAfter merge() variable features that have been calculated by SCTransform is reset.\n\n\n5.2.2 Plotting\nUse scCustomize for improved plotting over Seurat’s defaults.\nTo plot heatmaps with extra metadata information, use the scillius package.\nTo get statistics on differential gene levels, use the package ggbetweenstats from ggstatsplot.\n\ncell_expression &lt;- rna_integrated@assays$RNA@data[c(\"MIPOL1\", \"ETV1\", \"DGKB\", \"FOXA2\", \"AR\", \"SYP\"),] %&gt;% as.matrix() %&gt;% t()\nmeta_data &lt;- rna_integrated@meta.data\nplot_data &lt;- cbind(meta_data, cell_expression)\nsaveRDS(plot_data, \"scRNAseq_violin.RDS\")\n\n\n\n5.2.3 Utility functions\nRenaming all identities\n\ncell_types &lt;- c(\"0\" = \"\", \"1\" = \"\", \"2\" = \"\", \"3\" = \"\", \"4\" = \"\", \"5\" = \"\", \"6\" = \"\")\nIdents(merged) &lt;- \"SCT_snn_res.0.2\"\nmerged &lt;- RenameIdents(merged, cell_types)\nmerged$status &lt;- Idents(merged)\nDimPlot_scCustom(integrated, group.by = \"cell_types\", colors_use = polychrome_pal)\nmerged$status &lt;- factor(merged$status, \n                        levels = c(\"\")) # useful for rearranging levels\nIdents(merged) &lt;- \"status\"\n\nRenaming a few identities\n\nnew_groups &lt;- case_when(seu_obj$clust == \"a\" ~ \"Tumor\",\n          .default = seu_obj$clust) %&gt;% as.factor()\nnames(new_groups) &lt;- names(seu_obj$clust)\nseu_obj$new_groups &lt;- new_groups\nseu_obj$new_groups &lt;- factor(seu_obj$new_groups, \n                        levels = c(\"\")) # have to do this again\n\nAdding new clusters, cluster 13 was not present previously\n\nseu_obj$manual_clusters &lt;- seu_obj$integrated_snn_res.0.6\nlevels(seu_obj$manual_clusters) &lt;- c(levels(seu_obj$manual_clusters), \"13\")\nseu_obj$manual_clusters[names(seu_obj$manual_clusters) %in% select_cells] &lt;- \"13\""
  },
  {
    "objectID": "resources.html#bioinformatics",
    "href": "resources.html#bioinformatics",
    "title": "6  Books and Resources",
    "section": "6.1 Bioinformatics",
    "text": "6.1 Bioinformatics\nThe single most useful book in my bioinformatics career up to date is Bioinformatics Data Skills Buffalo (2015).\nI have not read Computational Genomics with R yet, but it looks extremely useful and actually might be a better summary of Chapter 2\nThe Babraham Institute has a very comprehensive training materials\nSingle-Cell Best Practices from the Theis Lab.\nEpigenomics Workshop by National Bioinformatics Infrastructure Sweden.\nhttps://bioinformaticsworkbook.org/list.html#gsc.tab=0\nhttps://docs.gdc.cancer.gov/Data/Introduction/"
  },
  {
    "objectID": "resources.html#programming",
    "href": "resources.html#programming",
    "title": "6  Books and Resources",
    "section": "6.2 Programming",
    "text": "6.2 Programming\n\n6.2.1 R\nThe best introduction book to R is unambigously https://r4ds.hadley.nz/. “The R Graph Gallery boasts the most extensive compilation of R-generated graphs on the web” https://r-graph-gallery.com/index.html\n\n\n6.2.2 Python\nhttps://wesmckinney.com/book/ https://www.rebeccabarter.com/blog/2023-09-11-from_r_to_python"
  },
  {
    "objectID": "resources.html#git",
    "href": "resources.html#git",
    "title": "6  Books and Resources",
    "section": "6.3 Git",
    "text": "6.3 Git\nhttps://happygitwithr.com/ is a comprehensive resource on how to use Git with R.\nIf you want to understand how git works more intuitively, watch this."
  },
  {
    "objectID": "resources.html#statistics",
    "href": "resources.html#statistics",
    "title": "6  Books and Resources",
    "section": "6.4 Statistics",
    "text": "6.4 Statistics\nStatQuest is a very accessible resource to those without a math background. I would recommend the Statistics Fundamentals and High Troughput Sequencing playlists.\nI haven’t read Modern Statistics for Modern Biology yet but it looks to be very relevant."
  },
  {
    "objectID": "resources.html#miscellanous",
    "href": "resources.html#miscellanous",
    "title": "6  Books and Resources",
    "section": "6.5 Miscellanous",
    "text": "6.5 Miscellanous\nThe Missing Semester of Your CS Education shows you how to master the command line and vim.\nAdobe Illustrator for Scientific Figures from the Raj Lab. [IGV handbook]https://www.igv.org/workshops/BroadApril2017/IGV_SlideDeck.pdf.\n\n\n\n\n\n\nBuffalo, V. 2015. Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. O’Reilly Media. https://books.google.com/books?id=XxERCgAAQBAJ."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Buffalo, V. 2015. Bioinformatics Data Skills: Reproducible and\nRobust Research with Open Source Tools. O’Reilly Media. https://books.google.com/books?id=XxERCgAAQBAJ."
  }
]