{
  "hash": "b15d79babe53d3c02a2238fd5319369d",
  "result": {
    "markdown": "---\nexecute:\n  eval: false\nformat:\n  html:\n    link-external-newwindow: true\n---\n\n\n# Glueing Together Pipelines\n\nThis chapter is named as such because I will be detailing the processing pipelines for our NGS data and the command line, i.e. the glue, that connects different parts together.\n\nAs of October 2023, we are starting to move ahead with switching to Nextflow and nf-core's processing pipelines which will take care of most basic processing steps such as mapping so I will not list them in too much detail.\n\n## Working with High Performance Computing (HPCs)\n\nNorthwestern's [Quest User Guide](https://services.northwestern.edu/TDClient/30/Portal/KB/ArticleDet?ID=505) is a great place to start with learning how HPCs work. However, with our own server at Emory, things are much simpler.\n\nCancel a lot of pending jobs `scancel --state PENDING --user vkp2256`.\n\n## Environment Reproducibility\n\nMany packages will now use [conda](https://conda.io/projects/conda/en/latest/user-guide/index.html), so many of those will be self-contained. I highly recommend using [mamba](https://mamba.readthedocs.io/en/latest/) to speed up installations.\n\nFor R, while one can use a conda environment, I've found it to be hard to set it up, so try to use only one version of R in one project to avoid dependency issues and do `sessioninfo()` to get package versions that were installed.\n\n## Nextflow\n\nNextflow is a pipeline construction language. Nf-core is a publicly contributed best-practices of bioinformatics pipelines. If you want to do basic, routine analysis, using nf-core's ready-made pipelines is best, but because of its large code-base and complexity, it might be hard to debug. You may want to start your own nextflow container if you find yourself doing similar analyses many times.\n\nUse `nextflow pull nf-core/rnaseq` to update pipelines. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since.\n\n## The Command Line\n\nIt is essential to master the basics of the command line. [@Buffalo_2015, Chapter 8] is a great place to get started. I found learning about path expansion, awk, and vim extremely useful. [The Missing Semester](https://www.youtube.com/watch?v=Z56Jmr9Z34Q) is also a great resource.\n\nMake your executable bash script have a help page by creating a [Help facility](https://opensource.com/article/19/12/help-bash-program).\n\nWhile I think going through these well-documented resources is better than anything I could share, I'll document the things I found really useful here.\n\n::: callout-warning\nSome of these might not be accurate, test carefully before using them (as you should do with any random piece of code you find).\n:::\n\n### One-liners\n\nThere are many great bioinformatics one-liner compilations such as [this one](https://github.com/stephenturner/oneliners). I suggest you keep a list of your own that you find yourself re-using often.\n\n### File manipulation\n\nRead in files line by line and perform an action with it.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nfile = \"/path/to/yourfile.txt\"\nwhile IFS= read -r line\ndo\necho \"$line\"\necho \"${line:8:5}\" # grab chars starting at 8th position(0 index) for 5 chars\ndone <\"$file\"\n```\n:::\n\n\nIf you want to refer to columns in your file\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Loop through each line in the mapping file and rename the files\nwhile IFS=' ' read -r old_name new_name; do\n    # Rename the files\n    mv \"$old_name\" \"$new_name\"\ndone < \"$mapping_file\"\n```\n:::\n\n\nChecking if a directory already exists\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nif [ ! -d $dir ]; then\n\tmkdir $dir\nfi\n```\n:::\n\n\nFor loops: there are many ways of specifying what to loop through\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\nfor i in m{1..3} m{13..15}\nfor i in 1 2 3 4 5\nfor i in file1 file2\n\ndo\necho \"Welcome $i times\"\nj=\"$(basename \"${i}\")\" # get the file name\n\nk=\"$(basename \"${i}\" | sed 's/_[^_]*$//')\" # prints file name without the extension and the last chunk after the last _\nl=\"$(dirname \"${i}\")\" # prints the dirname without filename\necho raw/JYu-${i}_*_L001* # filename expansion\n\ndone\n```\n:::\n\n\nFor more complex naming schemes, you may use an array.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Specify the last two digits of the numbers\nnumbers=(\"59\" \"82\" \"23\" \"45\" \"77\")\n\n# Iterate over the array\nfor num in \"${numbers[@]}\"; do\n  echo \"SRR13105$num\"\ndone\n\n# Iterate over indexes\nfor ((index=0;index<5;index++))\ndo\nnum=\"SRR13105${numbers[$index]}\"\ndone\n```\n:::\n\n\nSee [this](https://geekflare.com/bash-for-loop-examples/) for more examples and use cases.\n\n### Unix Text Processing: Awk and Sed\n\n**Awk**\n\nAwk statements are written as `pattern { action }`. If we omit the pattern, Awk will run the action on all records. If we omit the action but specify a pattern, Awk will print all records that match the pattern.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nawk '{if ($3 == \"sth) print $0;}' file.tsv\nawk -F \"\\t\" '{print NF; exit}' file.tsv # prints number of fields/columns\nawk -F \"\\t\" 'BEGIN{OFS=\"\\t\";} {print $1,$2,$3,\"+\"}' input.bed > output.bed # indicate that the output should be tab-delimited\ncolor=\"$(awk 'NR==val{print; exit}' val=$line color_list_3_chars.txt)\" # pick line based on counter and assigning to a variable\n```\n:::\n\n\n**Sed**\\\nSed statements are written as `substitute: 's/pattern/replacement/'`\n\nsed -e `some rule` -e `another rule` The g/ means global replace i.e. find all occurrences of foo and replace with bar using sed. If you removed the /g only first occurrence is changed. chaining rules also possible: sed -e 's/,/\\t/g ; 5q' print only the first 5 lines\n\n### Miscellanous\n\n-   Inside a bash script, you can use `$0` for the name of the script, `$1` for the first argument and so on.\\\n-   `$?` show error code of last command, 0 if everything went well, `$#` number of commands.\n-   `shuf -n 4 example_data.csv` print random 4 lines.\\\n-   `nohup command &` will make commmand run in the background, `fg` to bring it back to the foreground.\\\n-   https://www.shellcheck.net/ is a website where it can check for errors and bugs in your shell scripts.\n-   `paste -sd,` concatenate lines into a single line -s, delimited by a comma\n\n#### Working with lines\n\ncount number of occurrences of unique instances\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsort | uniq -c \n```\n:::\n\n\noutput number of duplicated lines if there are any\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nuniq -d mm_gene_names.txt | wc -l\n```\n:::\n\n\ncount fastq lines (assuming well-formatted fastq files which it usually are)\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncat file.fq | echo $((`wc -l`/4))\n```\n:::\n\n\nprint only the number of lines in a file and no filenames\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nwc -l m*/m*.narrowPeak | cut -f 4 -d' '\n```\n:::\n\n\nto get rid of x lines at the beginning of a file\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ntail -n +2 file.txt # starts from the 2nd line, i.e. removing the first line\n```\n:::\n\n\nto see the first 2 lines and the last 2 lines of a file\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n(head -n 2; tail -n 2) < Mus_musculus.GRCm38.75_chr1.bed\n```\n:::\n\n\n#### Working with columns\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncut -f 2 Mus_musculus.GRCm38.75_chr1.bed # only the second column\ncut -f 3-8 Mus_musculus.GRCm38.75_chr1.bed # range of columns  \ncut -f 3,6,7,8 Mus_musculus.GRCm38.75_chr1.bed # sets of columns, cannot be reordered  \ncut -d, -f2 some_file.csv # use comma as delimiter\n\ngrep -v \"^#\" some_file.gtf | cut -f 1-8 | column -t | head -n 3 #prettify output\n```\n:::\n\n\nremove header lines and count the number of columns\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ngrep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | awk -F \"\\t\" '{print NF; exit}'\n```\n:::\n\n\n#### Working with strings\n\nto remove everything after first dot. useful for getting sample name from filename.fastq.gz\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n${i%%.*}\n```\n:::\n\n\nto remove everything after last dot. useful for getting sample name from filename.param.fastq\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n${i%.*}\n```\n:::\n\n\nto remove everything before a /, including it\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n${i##*/}\n```\n:::\n\n\nto make uppercase\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n${var^}\n```\n:::\n\n\n#### Directories\n\nto get human readable size of folders under the parent folder\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ndu -h --max-depth=1 parent_folder_name\n```\n:::\n\n\ns means summary\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ndu -sh /full/path/directory\n```\n:::\n\n\nprint each file in new line\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nls -1\n```\n:::\n\n\ncheck what file takes the most space (d is one level down)\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ndu -d 1 | sort -n\n```\n:::\n\n\n#### Alignment files\n\nRemoving EBV chromosomes for viewing on UCSC browser.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsamtools idxstats m${i}_sorted.bam | cut -f 1 | grep -v 'chrEBV' | xargs samtools view -h m${i}_sorted.bam | grep -v 'chrEBV' | samtools view -o m${i}_filtered.bam\n```\n:::\n\n\nRemove reads not mapping to chr1-22, X, Y. this does not remove from headers. The first sed expression removes leading whitespace from echo (-n to ), the second expression to add \"chr\" at the beginning.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsamtools view -o mr392_filtered1.bam mr392_sorted.bam `echo -n {{1..22},X,Y}$'\\n' | sed -e 's/^[[:space:]]//' -e 's/^/chr/'`\n```\n:::\n\n\nor you can just chain together reverse grep to remove any chromosomes you want\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nchr=samtools view -H m1076_rgid_reheader_st.bam | grep chr | cut -f2 | sed 's/SN://g' | grep -v chrM | grep -v chrY | awk '{if(length($0)<6)print}'\n```\n:::\n\n\nthe awk statement is to remove the unknown chromosomes and random contigs since they will be longer than 6 chars\n\n\n::: {.cell}\n\n:::\n\n\n## Bioinformatics Bits\n\nThese are mostly still command line tools, but more bioinformatics related.\n\n### Getting files from GEO\n\n**Using SRAtools** <https://bioinformaticsworkbook.org/dataAcquisition/fileTransfer/sra.html#gsc.tab=0> To get the SRR runs numbers, use the SRA Run Selector. Select the ones you want to download or all of them, and download the Accession List. Upload to quest. Better to do a job array for each SRR. fasterq-dump is now the preferred and faster option. To download bam, can bystep sam step.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmodule load sratoolkit\nfastq-dump --gzip SRR # use --split-files for paired-end\nfasterq-dump SRR -O output/dir # for 10x, need to use --split-filles and --include-technical\nsam-dump SRR | samtools view -bS -> SRR.bam\n```\n:::\n\n\n**Using ftp**\\\nThis can be faster than SRAtoolkit but only works if those files have been uploaded to EBI.\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR101/006/SRR1016916\n\npattern: fastq/first 6 chars/00(last number)/full accession\n\n### Making UCSC tracks\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmdoule load homer\nmakeTagDirectory tag1045 1045.bed\nmakeUCSCfile tagm57 -o m57 -norm 2e7v\n```\n:::\n\n\nWith RNA-Seq UCSC tracks, use --fragLength given, otherwise it messes up the auto-correlation thinking that's it's ChIP-Seq leading to screwed up tracks.\n\nYou can upload the tracks and save it as a session that can be shared.\n\n### samtools\n\n[Learn the Bam Format](https://github.com/davetang/learning_bam_file).\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsamtools view -h test.bam # print bam with headers\nsamtools view -H test.bam # headers only\nsamtools flagstat test.bam # get mapping info\nsamtools idxstats test.bam\nsamtools -f 0x2 # read mapped in proper pair\nsamtools view -H test.bam | grep @HD # Check if BAM files are sorted\nsamtools view -F 4 test.bam | wc -l # check how many mapped reads\n```\n:::\n\n\n## R Quirks\n\nWhen saving anything produced with ggplot and you will be editing in Illustrator, set `useDingbats = FALSE`.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}